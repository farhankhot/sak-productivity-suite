{"ast":null,"code":"import _slicedToArray from\"C:/Users/farha/OneDrive/Desktop/sak-productivity-suite/client/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";/*global chrome*/import React,{useState,useEffect}from\"react\";import{jsx as _jsx}from\"react/jsx-runtime\";import{Fragment as _Fragment}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";function AudioRetrieval(){var _useState=useState(\"\"),_useState2=_slicedToArray(_useState,2),transcriptTextArea=_useState2[0],setTranscriptTextArea=_useState2[1];var handleStartCapture=function handleStartCapture(){chrome.tabCapture.capture({audio:true},function(stream){var audioContext=new AudioContext();var mediaStreamSource=audioContext.createMediaStreamSource(stream);mediaStreamSource.connect(audioContext.destination);// Set up the speech recognition object\nvar SpeechRecognition=window.speechRecognition||window.webkitSpeechRecognition;var recognition=new SpeechRecognition();recognition.continuous=true;recognition.interimResults=true;recognition.lang='en-US';recognition.listening=true;recognition.start();// Process the speech recognition results\nrecognition.onresult=function(event){var transcript=event.results[event.results.length-1][0].transcript;// Person stops talking\nsetTranscriptTextArea(transcriptTextArea+\" \"+transcript);};});};var handleTranscriptTextAreaChange=function handleTranscriptTextAreaChange(event){setTranscriptTextArea(event.target.value);};return/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"button\",{onClick:handleStartCapture,children:\"Start audio capture\"}),/*#__PURE__*/_jsx(\"textarea\",{value:transcriptTextArea,onChange:handleTranscriptTextAreaChange,placeholder:\"The transcript will appear here\"})]});}export default AudioRetrieval;","map":{"version":3,"names":["React","useState","useEffect","jsx","_jsx","Fragment","_Fragment","jsxs","_jsxs","AudioRetrieval","_useState","_useState2","_slicedToArray","transcriptTextArea","setTranscriptTextArea","handleStartCapture","chrome","tabCapture","capture","audio","stream","audioContext","AudioContext","mediaStreamSource","createMediaStreamSource","connect","destination","SpeechRecognition","window","speechRecognition","webkitSpeechRecognition","recognition","continuous","interimResults","lang","listening","start","onresult","event","transcript","results","length","handleTranscriptTextAreaChange","target","value","children","onClick","onChange","placeholder"],"sources":["C:/Users/farha/OneDrive/Desktop/sak-productivity-suite/client/src/AudioRetrieval.js"],"sourcesContent":["/*global chrome*/\r\nimport React, {useState, useEffect} from \"react\";\r\n\r\n\r\nfunction AudioRetrieval() {\r\n\t\r\n\tconst [transcriptTextArea, setTranscriptTextArea] = useState(\"\");\r\n\t\t\t\t\r\n\tconst handleStartCapture = () => {\r\n\t\t\r\n\t\tchrome.tabCapture.capture({audio: true}, function(stream) {\r\n\t\t\t\r\n\t\t\tconst audioContext = new AudioContext();\r\n\t\t\tconst mediaStreamSource = audioContext.createMediaStreamSource(stream);\r\n\t\t\tmediaStreamSource.connect(audioContext.destination);\r\n\r\n\t\t\t// Set up the speech recognition object\r\n\t\t\tconst SpeechRecognition = window.speechRecognition || window.webkitSpeechRecognition;\r\n\t\t\tconst recognition = new SpeechRecognition();\r\n\r\n\t\t\trecognition.continuous = true;\r\n\t\t\trecognition.interimResults = true;\r\n\t\t\trecognition.lang = 'en-US';\r\n\t\t\trecognition.listening = true;\r\n\t\t\t\r\n\t\t\trecognition.start();\r\n\r\n\t\t\t// Process the speech recognition results\r\n\t\t\trecognition.onresult = function(event) {\r\n\t\t\t\tconst transcript = event.results[event.results.length - 1][0].transcript;\r\n\t\t\t\t// Person stops talking\r\n\t\t\t\tsetTranscriptTextArea(transcriptTextArea + \" \" + transcript);\r\n\t\t\t}\r\n\t\t});\r\n\t\t\r\n\t};\r\n\t\r\n\tconst handleTranscriptTextAreaChange = (event) => {\r\n\t\tsetTranscriptTextArea(event.target.value);\r\n\t};\r\n\t\r\n\treturn (\r\n\t\t<>\r\n\t\t<button onClick={handleStartCapture}>\r\n\t\t\tStart audio capture\r\n\t\t</button>\r\n\t\t<textarea value={transcriptTextArea} onChange={handleTranscriptTextAreaChange} placeholder=\"The transcript will appear here\"></textarea>\r\n\t\t</>\r\n\t\t\r\n\t);\r\n\r\n}\r\nexport default AudioRetrieval;"],"mappings":"mJAAA,iBACA,MAAO,CAAAA,KAAK,EAAGC,QAAQ,CAAEC,SAAS,KAAO,OAAO,CAAC,OAAAC,GAAA,IAAAC,IAAA,gCAAAC,QAAA,IAAAC,SAAA,gCAAAC,IAAA,IAAAC,KAAA,yBAGjD,QAAS,CAAAC,cAAcA,CAAA,CAAG,CAEzB,IAAAC,SAAA,CAAoDT,QAAQ,CAAC,EAAE,CAAC,CAAAU,UAAA,CAAAC,cAAA,CAAAF,SAAA,IAAzDG,kBAAkB,CAAAF,UAAA,IAAEG,qBAAqB,CAAAH,UAAA,IAEhD,GAAM,CAAAI,kBAAkB,CAAG,QAArB,CAAAA,kBAAkBA,CAAA,CAAS,CAEhCC,MAAM,CAACC,UAAU,CAACC,OAAO,CAAC,CAACC,KAAK,CAAE,IAAI,CAAC,CAAE,SAASC,MAAM,CAAE,CAEzD,GAAM,CAAAC,YAAY,CAAG,GAAI,CAAAC,YAAY,EAAE,CACvC,GAAM,CAAAC,iBAAiB,CAAGF,YAAY,CAACG,uBAAuB,CAACJ,MAAM,CAAC,CACtEG,iBAAiB,CAACE,OAAO,CAACJ,YAAY,CAACK,WAAW,CAAC,CAEnD;AACA,GAAM,CAAAC,iBAAiB,CAAGC,MAAM,CAACC,iBAAiB,EAAID,MAAM,CAACE,uBAAuB,CACpF,GAAM,CAAAC,WAAW,CAAG,GAAI,CAAAJ,iBAAiB,EAAE,CAE3CI,WAAW,CAACC,UAAU,CAAG,IAAI,CAC7BD,WAAW,CAACE,cAAc,CAAG,IAAI,CACjCF,WAAW,CAACG,IAAI,CAAG,OAAO,CAC1BH,WAAW,CAACI,SAAS,CAAG,IAAI,CAE5BJ,WAAW,CAACK,KAAK,EAAE,CAEnB;AACAL,WAAW,CAACM,QAAQ,CAAG,SAASC,KAAK,CAAE,CACtC,GAAM,CAAAC,UAAU,CAAGD,KAAK,CAACE,OAAO,CAACF,KAAK,CAACE,OAAO,CAACC,MAAM,CAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAACF,UAAU,CACxE;AACAzB,qBAAqB,CAACD,kBAAkB,CAAG,GAAG,CAAG0B,UAAU,CAAC,CAC7D,CAAC,CACF,CAAC,CAAC,CAEH,CAAC,CAED,GAAM,CAAAG,8BAA8B,CAAG,QAAjC,CAAAA,8BAA8BA,CAAIJ,KAAK,CAAK,CACjDxB,qBAAqB,CAACwB,KAAK,CAACK,MAAM,CAACC,KAAK,CAAC,CAC1C,CAAC,CAED,mBACCpC,KAAA,CAAAF,SAAA,EAAAuC,QAAA,eACAzC,IAAA,WAAQ0C,OAAO,CAAE/B,kBAAmB,CAAA8B,QAAA,CAAC,qBAErC,EAAS,cACTzC,IAAA,aAAUwC,KAAK,CAAE/B,kBAAmB,CAACkC,QAAQ,CAAEL,8BAA+B,CAACM,WAAW,CAAC,iCAAiC,EAAY,GACrI,CAIL,CACA,cAAe,CAAAvC,cAAc"},"metadata":{},"sourceType":"module","externalDependencies":[]}