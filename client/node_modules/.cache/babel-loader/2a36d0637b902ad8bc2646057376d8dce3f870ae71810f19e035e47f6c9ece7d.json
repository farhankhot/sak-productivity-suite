{"ast":null,"code":"import _slicedToArray from\"C:/Users/farha/OneDrive/Desktop/sak-productivity-suite/client/node_modules/@babel/runtime/helpers/esm/slicedToArray.js\";/*global chrome*/import React,{useState,useEffect}from\"react\";import{jsx as _jsx}from\"react/jsx-runtime\";import{Fragment as _Fragment}from\"react/jsx-runtime\";import{jsxs as _jsxs}from\"react/jsx-runtime\";function AudioRetrieval(){var _useState=useState([]),_useState2=_slicedToArray(_useState,2),transcriptTextArea=_useState2[0],setTranscriptTextArea=_useState2[1];var handleStartCapture=function handleStartCapture(){// // Get the current tab's audio stream\n// chrome.tabCapture.capture({audio: true}, function(stream) {\n// const audioContext = new AudioContext();\n// // Create a MediaElementAudioSourceNode from the stream\n// const sourceNode = audioContext.createMediaStreamSource(stream);\n// // Create a ScriptProcessorNode for processing the audio data\n// const scriptNode = audioContext.createScriptProcessor(2048, 1, 1);\n// // Connect the nodes\n// sourceNode.connect(scriptNode);\n// scriptNode.connect(audioContext.destination);\n// Set up the speech recognition object\nvar SpeechRecognition=window.speechRecognition||window.webkitSpeechRecognition;var recognition=new SpeechRecognition();recognition.continuous=true;recognition.interimResults=true;recognition.lang='en-US';recognition.listening=true;// console.log(recognition);\n// Process the audio data in the scriptNode's onaudioprocess callback\n// scriptNode.onaudioprocess = function(audioProcessingEvent) {\n// const inputBuffer = audioProcessingEvent.inputBuffer;\n// // Process the audio data in the inputBuffer\n// // ...\n// try {\n// Start the speech recognition\nrecognition.start();// Process the speech recognition results\nrecognition.onresult=function(event){console.log(event);var transcript=event.results[event.results.length-1][0].transcript;console.log(transcript);setTranscriptTextArea(transcript);};// }catch(error){\n// console.log(error);\n// }\n// };\n// });\n};var handleTranscriptTextAreaChange=function handleTranscriptTextAreaChange(event){setTranscriptTextArea(event.target.value);};return/*#__PURE__*/_jsxs(_Fragment,{children:[/*#__PURE__*/_jsx(\"button\",{onClick:handleStartCapture,children:\"Start audio capture\"}),/*#__PURE__*/_jsx(\"textarea\",{value:transcriptTextArea,onChange:handleTranscriptTextAreaChange,placeholder:\"The transcript will appear here\"})]});}export default AudioRetrieval;","map":{"version":3,"names":["React","useState","useEffect","jsx","_jsx","Fragment","_Fragment","jsxs","_jsxs","AudioRetrieval","_useState","_useState2","_slicedToArray","transcriptTextArea","setTranscriptTextArea","handleStartCapture","SpeechRecognition","window","speechRecognition","webkitSpeechRecognition","recognition","continuous","interimResults","lang","listening","start","onresult","event","console","log","transcript","results","length","handleTranscriptTextAreaChange","target","value","children","onClick","onChange","placeholder"],"sources":["C:/Users/farha/OneDrive/Desktop/sak-productivity-suite/client/src/AudioRetrieval.js"],"sourcesContent":["/*global chrome*/\r\nimport React, {useState, useEffect} from \"react\";\r\n\r\n\r\nfunction AudioRetrieval() {\r\n\t\r\n\tconst [transcriptTextArea, setTranscriptTextArea] = useState([]);\r\n\t\t\t\t\r\n\tconst handleStartCapture = () => {\r\n\t\t// // Get the current tab's audio stream\r\n\t\t// chrome.tabCapture.capture({audio: true}, function(stream) {\r\n\r\n\t\t\t// const audioContext = new AudioContext();\r\n\r\n\t\t\t// // Create a MediaElementAudioSourceNode from the stream\r\n\t\t\t// const sourceNode = audioContext.createMediaStreamSource(stream);\r\n\r\n\t\t\t// // Create a ScriptProcessorNode for processing the audio data\r\n\t\t\t// const scriptNode = audioContext.createScriptProcessor(2048, 1, 1);\r\n\r\n\t\t\t// // Connect the nodes\r\n\t\t\t// sourceNode.connect(scriptNode);\r\n\t\t\t// scriptNode.connect(audioContext.destination);\r\n\r\n\t\t\t// Set up the speech recognition object\r\n\t\t\tconst SpeechRecognition = window.speechRecognition || window.webkitSpeechRecognition;\r\n\t\t\tconst recognition = new SpeechRecognition();\r\n\r\n\t\t\trecognition.continuous = true;\r\n\t\t\trecognition.interimResults = true;\r\n\t\t\trecognition.lang = 'en-US';\r\n\t\t\trecognition.listening = true;\r\n\r\n\t\t\t// console.log(recognition);\r\n\r\n\t\t\t// Process the audio data in the scriptNode's onaudioprocess callback\r\n\t\t\t// scriptNode.onaudioprocess = function(audioProcessingEvent) {\r\n\t\t\t\t// const inputBuffer = audioProcessingEvent.inputBuffer;\r\n\r\n\t\t\t\t// // Process the audio data in the inputBuffer\r\n\t\t\t\t// // ...\r\n\t\t\t\t// try {\r\n\t\t\t\t\t// Start the speech recognition\r\n\t\t\t\t\trecognition.start();\r\n\r\n\t\t\t\t\t// Process the speech recognition results\r\n\t\t\t\t\trecognition.onresult = function(event) {\r\n\t\t\t\t\t\tconsole.log(event);\r\n\t\t\t\t\t\tconst transcript = event.results[event.results.length - 1][0].transcript;\r\n\t\t\t\t\t\tconsole.log(transcript);\r\n\t\t\t\t\t\tsetTranscriptTextArea(transcript);\r\n\t\t\t\t\t}\r\n\t\t\t\t// }catch(error){\r\n\t\t\t\t\t// console.log(error);\r\n\t\t\t\t// }\r\n\t\t\t// };\r\n\r\n\t\t\t\r\n\t\t// });\r\n\t\t\r\n\t};\r\n\t\r\n\tconst handleTranscriptTextAreaChange = (event) => {\r\n\t\tsetTranscriptTextArea(event.target.value);\r\n\t};\r\n\t\r\n\treturn (\r\n\t\t<>\r\n\t\t<button onClick={handleStartCapture}>\r\n\t\t\tStart audio capture\r\n\t\t</button>\r\n\t\t<textarea value={transcriptTextArea} onChange={handleTranscriptTextAreaChange} placeholder=\"The transcript will appear here\"></textarea>\r\n\t\t</>\r\n\t\t\r\n\t);\r\n\r\n}\r\nexport default AudioRetrieval;"],"mappings":"mJAAA,iBACA,MAAO,CAAAA,KAAK,EAAGC,QAAQ,CAAEC,SAAS,KAAO,OAAO,CAAC,OAAAC,GAAA,IAAAC,IAAA,gCAAAC,QAAA,IAAAC,SAAA,gCAAAC,IAAA,IAAAC,KAAA,yBAGjD,QAAS,CAAAC,cAAcA,CAAA,CAAG,CAEzB,IAAAC,SAAA,CAAoDT,QAAQ,CAAC,EAAE,CAAC,CAAAU,UAAA,CAAAC,cAAA,CAAAF,SAAA,IAAzDG,kBAAkB,CAAAF,UAAA,IAAEG,qBAAqB,CAAAH,UAAA,IAEhD,GAAM,CAAAI,kBAAkB,CAAG,QAArB,CAAAA,kBAAkBA,CAAA,CAAS,CAChC;AACA;AAEC;AAEA;AACA;AAEA;AACA;AAEA;AACA;AACA;AAEA;AACA,GAAM,CAAAC,iBAAiB,CAAGC,MAAM,CAACC,iBAAiB,EAAID,MAAM,CAACE,uBAAuB,CACpF,GAAM,CAAAC,WAAW,CAAG,GAAI,CAAAJ,iBAAiB,EAAE,CAE3CI,WAAW,CAACC,UAAU,CAAG,IAAI,CAC7BD,WAAW,CAACE,cAAc,CAAG,IAAI,CACjCF,WAAW,CAACG,IAAI,CAAG,OAAO,CAC1BH,WAAW,CAACI,SAAS,CAAG,IAAI,CAE5B;AAEA;AACA;AACC;AAEA;AACA;AACA;AACC;AACAJ,WAAW,CAACK,KAAK,EAAE,CAEnB;AACAL,WAAW,CAACM,QAAQ,CAAG,SAASC,KAAK,CAAE,CACtCC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC,CAClB,GAAM,CAAAG,UAAU,CAAGH,KAAK,CAACI,OAAO,CAACJ,KAAK,CAACI,OAAO,CAACC,MAAM,CAAG,CAAC,CAAC,CAAC,CAAC,CAAC,CAACF,UAAU,CACxEF,OAAO,CAACC,GAAG,CAACC,UAAU,CAAC,CACvBhB,qBAAqB,CAACgB,UAAU,CAAC,CAClC,CAAC,CACF;AACC;AACD;AACD;AAGD;AAED,CAAC,CAED,GAAM,CAAAG,8BAA8B,CAAG,QAAjC,CAAAA,8BAA8BA,CAAIN,KAAK,CAAK,CACjDb,qBAAqB,CAACa,KAAK,CAACO,MAAM,CAACC,KAAK,CAAC,CAC1C,CAAC,CAED,mBACC3B,KAAA,CAAAF,SAAA,EAAA8B,QAAA,eACAhC,IAAA,WAAQiC,OAAO,CAAEtB,kBAAmB,CAAAqB,QAAA,CAAC,qBAErC,EAAS,cACThC,IAAA,aAAU+B,KAAK,CAAEtB,kBAAmB,CAACyB,QAAQ,CAAEL,8BAA+B,CAACM,WAAW,CAAC,iCAAiC,EAAY,GACrI,CAIL,CACA,cAAe,CAAA9B,cAAc"},"metadata":{},"sourceType":"module","externalDependencies":[]}