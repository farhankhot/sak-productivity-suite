{"ast":null,"code":"var _jsxFileName = \"C:\\\\Users\\\\farha\\\\OneDrive\\\\Documents\\\\sak-productivity-suite\\\\client\\\\src\\\\AudioRetrieval.js\",\n  _s = $RefreshSig$();\n/*global chrome*/\nimport React, { useState, useEffect, useRef } from \"react\";\nimport { jsxDEV as _jsxDEV } from \"react/jsx-dev-runtime\";\nimport { Fragment as _Fragment } from \"react/jsx-dev-runtime\";\nfunction AudioRetrieval() {\n  _s();\n  const [transcriptTextArea, setTranscriptTextArea] = useState(\"\");\n  const transcriptTextAreaRef = useRef(null);\n  const handleStartCapture = () => {\n    chrome.tabCapture.capture({\n      audio: true\n    }, function (stream) {\n      const audioContext = new AudioContext();\n      const mediaStreamSource = audioContext.createMediaStreamSource(stream);\n      mediaStreamSource.connect(audioContext.destination);\n\n      // Set up the speech recognition object\n      const SpeechRecognition = window.speechRecognition || window.webkitSpeechRecognition;\n      const recognition = new SpeechRecognition();\n      recognition.continuous = true;\n      recognition.interimResults = true;\n      recognition.lang = 'en-US';\n      recognition.listening = true;\n      recognition.start();\n\n      // Process the speech recognition results\n      recognition.onresult = function (event) {\n        let eventText = \"\";\n        for (let i = 0; i < event.results.length; i++) {\n          eventText += event.results[i][0].transcript;\n        }\n        // console.log(eventText);\n        transcriptTextAreaRef.current.value = eventText;\n      };\n    });\n  };\n  const handleSendingToAI = async () => {\n    const prompt = \"This is a conversation with a person: \" + \" Answer in a sentence or two. Do not be verbose: \" + transcriptTextAreaRef.current.value;\n    try {\n      const response = await fetch(\"https://sak-productivity-suite.herokuapp.com/use-chatgpt\", {\n        method: \"POST\",\n        headers: {\n          \"Content-Type\": \"application/json\"\n        },\n        body: JSON.stringify({\n          prompt: prompt\n        })\n      });\n      const data = await response.json();\n      setTranscriptTextArea(data.message);\n    } catch (error) {\n      console.log(error);\n    }\n  };\n  const handleTranscriptTextAreaChange = event => {\n    setTranscriptTextArea(event.target.value);\n  };\n  return /*#__PURE__*/_jsxDEV(_Fragment, {\n    children: [/*#__PURE__*/_jsxDEV(\"button\", {\n      onClick: handleStartCapture,\n      children: \"Start audio capture\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 74,\n      columnNumber: 3\n    }, this), /*#__PURE__*/_jsxDEV(\"button\", {\n      onClick: handleSendingToAI,\n      children: \"Send to AI\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 77,\n      columnNumber: 3\n    }, this), /*#__PURE__*/_jsxDEV(\"textarea\", {\n      ref: transcriptTextAreaRef,\n      value: transcriptTextArea,\n      onChange: handleTranscriptTextAreaChange,\n      placeholder: \"The transcript will appear here\"\n    }, void 0, false, {\n      fileName: _jsxFileName,\n      lineNumber: 80,\n      columnNumber: 3\n    }, this)]\n  }, void 0, true);\n}\n_s(AudioRetrieval, \"ETcj7Foegk5NpkcfR6YqNvZdfSs=\");\n_c = AudioRetrieval;\nexport default AudioRetrieval;\nvar _c;\n$RefreshReg$(_c, \"AudioRetrieval\");","map":{"version":3,"names":["React","useState","useEffect","useRef","jsxDEV","_jsxDEV","Fragment","_Fragment","AudioRetrieval","_s","transcriptTextArea","setTranscriptTextArea","transcriptTextAreaRef","handleStartCapture","chrome","tabCapture","capture","audio","stream","audioContext","AudioContext","mediaStreamSource","createMediaStreamSource","connect","destination","SpeechRecognition","window","speechRecognition","webkitSpeechRecognition","recognition","continuous","interimResults","lang","listening","start","onresult","event","eventText","i","results","length","transcript","current","value","handleSendingToAI","prompt","response","fetch","method","headers","body","JSON","stringify","data","json","message","error","console","log","handleTranscriptTextAreaChange","target","children","onClick","fileName","_jsxFileName","lineNumber","columnNumber","ref","onChange","placeholder","_c","$RefreshReg$"],"sources":["C:/Users/farha/OneDrive/Documents/sak-productivity-suite/client/src/AudioRetrieval.js"],"sourcesContent":["/*global chrome*/\r\nimport React, {useState, useEffect, useRef} from \"react\";\r\n\r\nfunction AudioRetrieval() {\r\n\t\r\n\tconst [transcriptTextArea, setTranscriptTextArea] = useState(\"\");\r\n\tconst transcriptTextAreaRef = useRef(null);\r\n\t\r\n\tconst handleStartCapture = () => {\r\n\t\t\r\n\t\tchrome.tabCapture.capture({audio: true}, function(stream) {\r\n\t\t\t\r\n\t\t\tconst audioContext = new AudioContext();\r\n\r\n\t\t\tconst mediaStreamSource = audioContext.createMediaStreamSource(stream);\r\n\t\t\tmediaStreamSource.connect(audioContext.destination);\r\n\r\n\t\t\t// Set up the speech recognition object\r\n\t\t\tconst SpeechRecognition = window.speechRecognition || window.webkitSpeechRecognition;\r\n\t\t\tconst recognition = new SpeechRecognition();\r\n\t\t\t\r\n\t\t\trecognition.continuous = true;\r\n\t\t\trecognition.interimResults = true;\r\n\t\t\trecognition.lang = 'en-US';\r\n\t\t\trecognition.listening = true;\r\n\t\t\t\r\n\t\t\trecognition.start();\r\n\r\n\t\t\t// Process the speech recognition results\r\n\t\t\trecognition.onresult = function(event) {\r\n\t\t\t\tlet eventText = \"\";\r\n\t\t\t\tfor (let i = 0; i < event.results.length; i++) {\r\n\t\t\t\t\teventText += event.results[i][0].transcript;\r\n\t\t\t\t}\r\n\t\t\t\t// console.log(eventText);\r\n\t\t\t\ttranscriptTextAreaRef.current.value = eventText;\r\n\t\t\t}\r\n\t\t\t\t\t\t\t\t\r\n\t\t});\r\n\t\t\r\n\t};\r\n\t\r\n\tconst handleSendingToAI = async() => {\r\n\t\tconst prompt = \"This is a conversation with a person: \"\r\n\t\t+ \" Answer in a sentence or two. Do not be verbose: \" + transcriptTextAreaRef.current.value;\r\n\t\t\r\n\t\ttry {\r\n\t\t\tconst response = await fetch(\"https://sak-productivity-suite.herokuapp.com/use-chatgpt\", {\r\n\t\t\t\tmethod: \"POST\",\r\n\t\t\t\theaders: {\r\n\t\t\t\t\t\"Content-Type\": \"application/json\"\r\n\t\t\t\t},\r\n\t\t\t\tbody: JSON.stringify({\r\n\t\t\t\t\tprompt: prompt\r\n\t\t\t\t})\r\n\t\t\t});\r\n\t\t\t\r\n\t\t\tconst data = await response.json();\t\r\n\t\t\tsetTranscriptTextArea(data.message);\t\t\t\r\n\r\n\t\t}catch(error){\r\n\t\t\tconsole.log(error);\r\n\t\t}\r\n\r\n\t};\r\n\t\r\n\t\t\r\n\tconst handleTranscriptTextAreaChange = (event) => {\r\n\t\tsetTranscriptTextArea(event.target.value);\r\n\t};\r\n\t\r\n\treturn (\r\n\t\t<>\r\n\t\t<button onClick={handleStartCapture}>\r\n\t\t\tStart audio capture\r\n\t\t</button>\r\n\t\t<button onClick={handleSendingToAI}>\r\n\t\t\tSend to AI\r\n\t\t</button>\r\n\t\t<textarea ref={transcriptTextAreaRef} value={transcriptTextArea} onChange={handleTranscriptTextAreaChange} placeholder=\"The transcript will appear here\"></textarea>\r\n\t\t</>\r\n\t);\r\n\r\n}\r\nexport default AudioRetrieval;"],"mappings":";;AAAA;AACA,OAAOA,KAAK,IAAGC,QAAQ,EAAEC,SAAS,EAAEC,MAAM,QAAO,OAAO;AAAC,SAAAC,MAAA,IAAAC,OAAA;AAAA,SAAAC,QAAA,IAAAC,SAAA;AAEzD,SAASC,cAAcA,CAAA,EAAG;EAAAC,EAAA;EAEzB,MAAM,CAACC,kBAAkB,EAAEC,qBAAqB,CAAC,GAAGV,QAAQ,CAAC,EAAE,CAAC;EAChE,MAAMW,qBAAqB,GAAGT,MAAM,CAAC,IAAI,CAAC;EAE1C,MAAMU,kBAAkB,GAAGA,CAAA,KAAM;IAEhCC,MAAM,CAACC,UAAU,CAACC,OAAO,CAAC;MAACC,KAAK,EAAE;IAAI,CAAC,EAAE,UAASC,MAAM,EAAE;MAEzD,MAAMC,YAAY,GAAG,IAAIC,YAAY,EAAE;MAEvC,MAAMC,iBAAiB,GAAGF,YAAY,CAACG,uBAAuB,CAACJ,MAAM,CAAC;MACtEG,iBAAiB,CAACE,OAAO,CAACJ,YAAY,CAACK,WAAW,CAAC;;MAEnD;MACA,MAAMC,iBAAiB,GAAGC,MAAM,CAACC,iBAAiB,IAAID,MAAM,CAACE,uBAAuB;MACpF,MAAMC,WAAW,GAAG,IAAIJ,iBAAiB,EAAE;MAE3CI,WAAW,CAACC,UAAU,GAAG,IAAI;MAC7BD,WAAW,CAACE,cAAc,GAAG,IAAI;MACjCF,WAAW,CAACG,IAAI,GAAG,OAAO;MAC1BH,WAAW,CAACI,SAAS,GAAG,IAAI;MAE5BJ,WAAW,CAACK,KAAK,EAAE;;MAEnB;MACAL,WAAW,CAACM,QAAQ,GAAG,UAASC,KAAK,EAAE;QACtC,IAAIC,SAAS,GAAG,EAAE;QAClB,KAAK,IAAIC,CAAC,GAAG,CAAC,EAAEA,CAAC,GAAGF,KAAK,CAACG,OAAO,CAACC,MAAM,EAAEF,CAAC,EAAE,EAAE;UAC9CD,SAAS,IAAID,KAAK,CAACG,OAAO,CAACD,CAAC,CAAC,CAAC,CAAC,CAAC,CAACG,UAAU;QAC5C;QACA;QACA7B,qBAAqB,CAAC8B,OAAO,CAACC,KAAK,GAAGN,SAAS;MAChD,CAAC;IAEF,CAAC,CAAC;EAEH,CAAC;EAED,MAAMO,iBAAiB,GAAG,MAAAA,CAAA,KAAW;IACpC,MAAMC,MAAM,GAAG,wCAAwC,GACrD,mDAAmD,GAAGjC,qBAAqB,CAAC8B,OAAO,CAACC,KAAK;IAE3F,IAAI;MACH,MAAMG,QAAQ,GAAG,MAAMC,KAAK,CAAC,0DAA0D,EAAE;QACxFC,MAAM,EAAE,MAAM;QACdC,OAAO,EAAE;UACR,cAAc,EAAE;QACjB,CAAC;QACDC,IAAI,EAAEC,IAAI,CAACC,SAAS,CAAC;UACpBP,MAAM,EAAEA;QACT,CAAC;MACF,CAAC,CAAC;MAEF,MAAMQ,IAAI,GAAG,MAAMP,QAAQ,CAACQ,IAAI,EAAE;MAClC3C,qBAAqB,CAAC0C,IAAI,CAACE,OAAO,CAAC;IAEpC,CAAC,QAAMC,KAAK,EAAC;MACZC,OAAO,CAACC,GAAG,CAACF,KAAK,CAAC;IACnB;EAED,CAAC;EAGD,MAAMG,8BAA8B,GAAIvB,KAAK,IAAK;IACjDzB,qBAAqB,CAACyB,KAAK,CAACwB,MAAM,CAACjB,KAAK,CAAC;EAC1C,CAAC;EAED,oBACCtC,OAAA,CAAAE,SAAA;IAAAsD,QAAA,gBACAxD,OAAA;MAAQyD,OAAO,EAAEjD,kBAAmB;MAAAgD,QAAA,EAAC;IAErC;MAAAE,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,QAAS,eACT7D,OAAA;MAAQyD,OAAO,EAAElB,iBAAkB;MAAAiB,QAAA,EAAC;IAEpC;MAAAE,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,QAAS,eACT7D,OAAA;MAAU8D,GAAG,EAAEvD,qBAAsB;MAAC+B,KAAK,EAAEjC,kBAAmB;MAAC0D,QAAQ,EAAET,8BAA+B;MAACU,WAAW,EAAC;IAAiC;MAAAN,QAAA,EAAAC,YAAA;MAAAC,UAAA;MAAAC,YAAA;IAAA,QAAY;EAAA,gBACjK;AAGL;AAACzD,EAAA,CAhFQD,cAAc;AAAA8D,EAAA,GAAd9D,cAAc;AAiFvB,eAAeA,cAAc;AAAC,IAAA8D,EAAA;AAAAC,YAAA,CAAAD,EAAA"},"metadata":{},"sourceType":"module","externalDependencies":[]}